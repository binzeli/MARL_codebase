class TwoDimWorld:
    def __init__(self, rows, columns, goal, agents, alpha, gamma):
        self.rows = rows
        self.columns = columns
        self.goal = goal
        self.agents = agents
        self.alpha = alpha
        self.gamma = gamma
        self.stateSpace = [(row, column) for row in range(rows) for column in range(columns)]
        self.actionSpace = ['up', 'down', 'left', 'right', 'stay']

    def reset(self):
        # Resets the environment to the initial state for each agent
        self.agent_states = [(0, 0) for _ in range(len(self.agents))]
        return self.agent_states

    def step(self, actions):
        # Computes the next state for each agent based on their actions
        next_states = [self.transition(state, action) for state, action in zip(self.agent_states, actions)]
        rewards = [self.compute_reward(state, action) for state, action in zip(next_states, actions)]
        dones = [state == self.goal for state in next_states]
        self.agent_states = next_states
        return next_states, rewards, dones

    def transition(self, state, action):
        # State transition logic based on the action
        row, col = state
        if action == 'up': row = max(row - 1, 0)
        elif action == 'down': row = min(row + 1, self.rows - 1)
        elif action == 'left': col = max(col - 1, 0)
        elif action == 'right': col = min(col + 1, self.columns - 1)
        return (row, col)

    def compute_reward(self, state, action):
        # Computes the reward based on the state and action
        if state == self.goal:
            return 100
        elif action == 'stay':
            return 0
        else:
            return -1

    def getAllAgentInitialStates(self):
        return self.reset()

    def isTerminal(self, states):
        return all(state == self.goal for state in states)

    def getAllAgentActions(self, states, episode):
        return [agent.choose_action(state) for agent, state in zip(self.agents, states)]

    def getAllAgentRewards(self, actions, next_states):
        return [self.compute_reward(state, action) for state, action in zip(next_states, actions)]
